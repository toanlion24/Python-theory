{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä WORKING WITH STRUCTURED DATA\n",
    "# (L√†m vi·ªác v·ªõi D·ªØ li·ªáu c√≥ C·∫•u tr√∫c)\n",
    "\n",
    "---\n",
    "\n",
    "## M·ª•c l·ª•c\n",
    "1. [Gi·ªõi thi·ªáu v·ªÅ Structured Data](#1-gi·ªõi-thi·ªáu-v·ªÅ-structured-data)\n",
    "2. [JSON - JavaScript Object Notation](#2-json---javascript-object-notation)\n",
    "3. [CSV - Comma Separated Values](#3-csv---comma-separated-values)\n",
    "4. [XML - eXtensible Markup Language](#4-xml---extensible-markup-language)\n",
    "5. [YAML - YAML Ain't Markup Language](#5-yaml---yaml-aint-markup-language)\n",
    "6. [INI/Config Files](#6-iniconfig-files)\n",
    "7. [Data Serialization v·ªõi Pickle](#7-data-serialization-v·ªõi-pickle)\n",
    "8. [L√†m vi·ªác v·ªõi APIs](#8-l√†m-vi·ªác-v·ªõi-apis)\n",
    "9. [Data Validation](#9-data-validation)\n",
    "10. [·ª®ng d·ª•ng th·ª±c t·∫ø](#10-·ª©ng-d·ª•ng-th·ª±c-t·∫ø)\n",
    "11. [B√†i t·∫≠p th·ª±c h√†nh](#11-b√†i-t·∫≠p-th·ª±c-h√†nh)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gi·ªõi thi·ªáu v·ªÅ Structured Data\n",
    "\n",
    "### Structured Data l√† g√¨?\n",
    "\n",
    "**D·ªØ li·ªáu c√≥ c·∫•u tr√∫c** l√† d·ªØ li·ªáu ƒë∆∞·ª£c t·ªï ch·ª©c theo format nh·∫•t ƒë·ªãnh, c√≥ th·ªÉ d·ªÖ d√†ng ƒë·ªçc, ghi v√† x·ª≠ l√Ω b·∫±ng ch∆∞∆°ng tr√¨nh.\n",
    "\n",
    "### C√°c ƒë·ªãnh d·∫°ng ph·ªï bi·∫øn:\n",
    "\n",
    "| Format | M√¥ t·∫£ | ·ª®ng d·ª•ng |\n",
    "|--------|-------|----------|\n",
    "| **JSON** | Key-value, lightweight | APIs, Web, Config |\n",
    "| **CSV** | D·∫°ng b·∫£ng, ƒë∆°n gi·∫£n | Excel, Database, Reports |\n",
    "| **XML** | Hierarchical, tags | Enterprise, SOAP APIs |\n",
    "| **YAML** | Human-readable | Config files, DevOps |\n",
    "| **INI** | Sections, key=value | Windows config |\n",
    "| **Pickle** | Python objects | Caching, ML models |\n",
    "\n",
    "### So s√°nh c√°c format:\n",
    "\n",
    "```\n",
    "                 JSON      CSV       XML       YAML\n",
    "ƒê·ªçc ƒë∆∞·ª£c         ‚úÖ        ‚úÖ        ‚ö†Ô∏è        ‚úÖ‚úÖ\n",
    "K√≠ch th∆∞·ªõc       Nh·ªè       R·∫•t nh·ªè   L·ªõn       Nh·ªè\n",
    "Nested data      ‚úÖ        ‚ùå        ‚úÖ        ‚úÖ\n",
    "Schema           ‚ùå        ‚ùå        ‚úÖ        ‚ùå\n",
    "Comments         ‚ùå        ‚ùå        ‚úÖ        ‚úÖ\n",
    "Web friendly     ‚úÖ‚úÖ      ‚ö†Ô∏è        ‚ö†Ô∏è        ‚ùå\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# T·∫†O TH∆Ø M·ª§C L√ÄM VI·ªÜC\n",
    "# ========================================\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "print(\"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c 'data'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. JSON - JavaScript Object Notation\n",
    "\n",
    "**JSON** l√† format ph·ªï bi·∫øn nh·∫•t cho web APIs v√† config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON C∆† B·∫¢N\n",
    "# ========================================\n",
    "import json\n",
    "\n",
    "# Python dict ‚Üí JSON string (dumps)\n",
    "data = {\n",
    "    \"name\": \"Nguy·ªÖn VƒÉn An\",\n",
    "    \"age\": 25,\n",
    "    \"is_student\": True,\n",
    "    \"courses\": [\"Python\", \"Data Science\", \"Machine Learning\"],\n",
    "    \"address\": {\n",
    "        \"city\": \"H√† N·ªôi\",\n",
    "        \"district\": \"C·∫ßu Gi·∫•y\"\n",
    "    },\n",
    "    \"gpa\": None\n",
    "}\n",
    "\n",
    "# Chuy·ªÉn sang JSON string\n",
    "json_string = json.dumps(data, indent=2, ensure_ascii=False)\n",
    "print(\"=== Python ‚Üí JSON ===\")\n",
    "print(json_string)\n",
    "print(f\"\\nType: {type(json_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON STRING ‚Üí PYTHON DICT\n",
    "# ========================================\n",
    "\n",
    "json_text = '''\n",
    "{\n",
    "    \"product\": \"Laptop\",\n",
    "    \"price\": 15000000,\n",
    "    \"in_stock\": true,\n",
    "    \"specs\": {\n",
    "        \"cpu\": \"Intel i7\",\n",
    "        \"ram\": \"16GB\"\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse JSON string ‚Üí Python dict\n",
    "product = json.loads(json_text)\n",
    "\n",
    "print(\"=== JSON ‚Üí Python ===\")\n",
    "print(f\"Type: {type(product)}\")\n",
    "print(f\"Product: {product['product']}\")\n",
    "print(f\"Price: {product['price']:,} VNƒê\")\n",
    "print(f\"CPU: {product['specs']['cpu']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# GHI JSON V√ÄO FILE\n",
    "# ========================================\n",
    "\n",
    "students = [\n",
    "    {\"id\": 1, \"name\": \"An\", \"gpa\": 8.5},\n",
    "    {\"id\": 2, \"name\": \"B√¨nh\", \"gpa\": 9.0},\n",
    "    {\"id\": 3, \"name\": \"C∆∞·ªùng\", \"gpa\": 7.8}\n",
    "]\n",
    "\n",
    "# Ghi v√†o file\n",
    "with open('data/students.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(students, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ghi file data/students.json\")\n",
    "\n",
    "# ƒê·ªçc l·∫°i\n",
    "with open('data/students.json', 'r', encoding='utf-8') as f:\n",
    "    loaded = json.load(f)\n",
    "\n",
    "print(\"\\nN·ªôi dung file:\")\n",
    "for student in loaded:\n",
    "    print(f\"  {student['name']}: GPA {student['gpa']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON OPTIONS\n",
    "# ========================================\n",
    "\n",
    "data = {\"name\": \"Vi·ªát Nam\", \"cities\": [\"H√† N·ªôi\", \"TP.HCM\"]}\n",
    "\n",
    "print(\"=== JSON dump options ===\")\n",
    "\n",
    "# Default (ASCII escape)\n",
    "print(\"\\n1. Default:\")\n",
    "print(json.dumps(data))\n",
    "\n",
    "# ensure_ascii=False (gi·ªØ Unicode)\n",
    "print(\"\\n2. ensure_ascii=False:\")\n",
    "print(json.dumps(data, ensure_ascii=False))\n",
    "\n",
    "# indent (format ƒë·∫πp)\n",
    "print(\"\\n3. indent=2:\")\n",
    "print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "\n",
    "# sort_keys (s·∫Øp x·∫øp keys)\n",
    "print(\"\\n4. sort_keys=True:\")\n",
    "print(json.dumps(data, sort_keys=True, ensure_ascii=False))\n",
    "\n",
    "# separators (t√πy ch·ªânh separator)\n",
    "print(\"\\n5. Compact (no spaces):\")\n",
    "print(json.dumps(data, separators=(',', ':'), ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# X·ª¨ L√ù KI·ªÇU D·ªÆ LI·ªÜU ƒê·∫∂C BI·ªÜT\n",
    "# ========================================\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# Custom encoder cho c√°c ki·ªÉu kh√¥ng h·ªó tr·ª£ s·∫µn\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (datetime, date)):\n",
    "            return obj.isoformat()\n",
    "        if isinstance(obj, Decimal):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        if isinstance(obj, bytes):\n",
    "            return obj.decode('utf-8')\n",
    "        return super().default(obj)\n",
    "\n",
    "# Data v·ªõi ki·ªÉu ƒë·∫∑c bi·ªát\n",
    "special_data = {\n",
    "    \"timestamp\": datetime.now(),\n",
    "    \"date\": date.today(),\n",
    "    \"price\": Decimal(\"99.99\"),\n",
    "    \"tags\": {\"python\", \"json\", \"data\"},\n",
    "    \"binary\": b\"hello\"\n",
    "}\n",
    "\n",
    "print(\"=== Custom Encoder ===\")\n",
    "result = json.dumps(special_data, cls=CustomEncoder, indent=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# X·ª¨ L√ù JSON ERRORS\n",
    "# ========================================\n",
    "\n",
    "invalid_json_samples = [\n",
    "    '{\"name\": \"An\",}',           # Trailing comma\n",
    "    \"{'name': 'An'}\",             # Single quotes\n",
    "    '{name: \"An\"}',               # Unquoted key\n",
    "    '{\"name\": undefined}',        # undefined not allowed\n",
    "]\n",
    "\n",
    "print(\"=== JSON Errors ===\")\n",
    "\n",
    "for json_str in invalid_json_samples:\n",
    "    try:\n",
    "        json.loads(json_str)\n",
    "        print(f\"‚úÖ Valid: {json_str}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Invalid: {json_str[:30]}...\")\n",
    "        print(f\"   Error: {e.msg} at position {e.pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON PATH - Truy c·∫≠p nested data\n",
    "# ========================================\n",
    "\n",
    "def json_get(data, path, default=None):\n",
    "    \"\"\"Truy c·∫≠p nested JSON data an to√†n\n",
    "    \n",
    "    Args:\n",
    "        data: JSON data (dict/list)\n",
    "        path: \"key1.key2.0.key3\" ho·∫∑c [\"key1\", \"key2\", 0, \"key3\"]\n",
    "        default: Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        path = path.replace('[', '.').replace(']', '').split('.')\n",
    "    \n",
    "    result = data\n",
    "    try:\n",
    "        for key in path:\n",
    "            if key == '':\n",
    "                continue\n",
    "            if isinstance(result, list):\n",
    "                result = result[int(key)]\n",
    "            else:\n",
    "                result = result[key]\n",
    "        return result\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        return default\n",
    "\n",
    "# Complex JSON data\n",
    "api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"data\": {\n",
    "        \"users\": [\n",
    "            {\"id\": 1, \"name\": \"An\", \"email\": \"an@test.com\"},\n",
    "            {\"id\": 2, \"name\": \"B√¨nh\", \"email\": \"binh@test.com\"}\n",
    "        ],\n",
    "        \"pagination\": {\n",
    "            \"page\": 1,\n",
    "            \"total\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== JSON Path ===\")\n",
    "print(f\"status: {json_get(api_response, 'status')}\")\n",
    "print(f\"first user: {json_get(api_response, 'data.users.0.name')}\")\n",
    "print(f\"second email: {json_get(api_response, 'data.users.1.email')}\")\n",
    "print(f\"total: {json_get(api_response, 'data.pagination.total')}\")\n",
    "print(f\"not exist: {json_get(api_response, 'data.missing', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CSV - Comma Separated Values\n",
    "\n",
    "**CSV** l√† format ƒë∆°n gi·∫£n cho d·ªØ li·ªáu d·∫°ng b·∫£ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CSV C∆† B·∫¢N V·ªöI csv MODULE\n",
    "# ========================================\n",
    "import csv\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu\n",
    "employees = [\n",
    "    [\"ID\", \"Name\", \"Department\", \"Salary\"],\n",
    "    [1, \"Nguy·ªÖn VƒÉn An\", \"IT\", 15000000],\n",
    "    [2, \"Tr·∫ßn Th·ªã B√¨nh\", \"HR\", 12000000],\n",
    "    [3, \"L√™ VƒÉn C∆∞·ªùng\", \"Sales\", 18000000],\n",
    "    [4, \"Ph·∫°m Th·ªã Dung\", \"IT\", 16000000]\n",
    "]\n",
    "\n",
    "# Ghi CSV\n",
    "with open('data/employees.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(employees)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ghi file employees.csv\")\n",
    "\n",
    "# ƒê·ªçc CSV\n",
    "print(\"\\nƒê·ªçc file:\")\n",
    "with open('data/employees.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DICTREADER V√Ä DICTWRITER\n",
    "# ========================================\n",
    "\n",
    "# DictWriter - Ghi t·ª´ list of dicts\n",
    "products = [\n",
    "    {\"id\": 1, \"name\": \"Laptop\", \"price\": 15000000, \"stock\": 50},\n",
    "    {\"id\": 2, \"name\": \"Mouse\", \"price\": 500000, \"stock\": 200},\n",
    "    {\"id\": 3, \"name\": \"Keyboard\", \"price\": 1200000, \"stock\": 150},\n",
    "]\n",
    "\n",
    "with open('data/products.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    fieldnames = ['id', 'name', 'price', 'stock']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    writer.writerows(products)\n",
    "\n",
    "print(\"=== DictWriter ===\")\n",
    "print(\"‚úÖ ƒê√£ ghi products.csv\")\n",
    "\n",
    "# DictReader - ƒê·ªçc th√†nh dicts\n",
    "print(\"\\n=== DictReader ===\")\n",
    "with open('data/products.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    print(f\"Headers: {reader.fieldnames}\")\n",
    "    print()\n",
    "    for row in reader:\n",
    "        print(f\"  {row['name']}: {int(row['price']):,} VNƒê (stock: {row['stock']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CSV V·ªöI DELIMITER KH√ÅC\n",
    "# ========================================\n",
    "\n",
    "# Tab-separated (TSV)\n",
    "tsv_data = [\n",
    "    [\"Name\", \"Age\", \"City\"],\n",
    "    [\"An\", 25, \"Hanoi\"],\n",
    "    [\"B√¨nh\", 30, \"HCM\"]\n",
    "]\n",
    "\n",
    "# Ghi TSV\n",
    "with open('data/data.tsv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(tsv_data)\n",
    "\n",
    "print(\"=== TSV (Tab-separated) ===\")\n",
    "with open('data/data.tsv', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Semicolon-separated (ph·ªï bi·∫øn ·ªü ch√¢u √Çu)\n",
    "with open('data/data_eu.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=';')\n",
    "    writer.writerows(tsv_data)\n",
    "\n",
    "print(\"=== Semicolon-separated ===\")\n",
    "with open('data/data_eu.csv', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# X·ª¨ L√ù CSV V·ªöI SPECIAL CHARACTERS\n",
    "# ========================================\n",
    "\n",
    "# Data v·ªõi d·∫•u ph·∫©y, xu·ªëng d√≤ng\n",
    "special_data = [\n",
    "    [\"Name\", \"Address\", \"Notes\"],\n",
    "    [\"An\", \"123 Main St, Suite 100\", \"VIP customer\"],\n",
    "    [\"B√¨nh\", \"456 Oak Ave\", \"Notes with\\nnewline\"],\n",
    "    ['C∆∞·ªùng', 'He said \"Hello\"', 'Quoted text']\n",
    "]\n",
    "\n",
    "with open('data/special.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(special_data)\n",
    "\n",
    "print(\"=== CSV v·ªõi special characters ===\")\n",
    "with open('data/special.csv', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# ƒê·ªçc l·∫°i\n",
    "print(\"\\nƒê·ªçc l·∫°i:\")\n",
    "with open('data/special.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CSV V·ªöI PANDAS (Khuy·∫øn kh√≠ch)\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # ƒê·ªçc CSV\n",
    "    df = pd.read_csv('data/employees.csv')\n",
    "    \n",
    "    print(\"=== Pandas DataFrame ===\")\n",
    "    print(df)\n",
    "    \n",
    "    # Th·ªëng k√™\n",
    "    print(f\"\\nL∆∞∆°ng trung b√¨nh: {df['Salary'].mean():,.0f} VNƒê\")\n",
    "    print(f\"T·ªïng l∆∞∆°ng: {df['Salary'].sum():,.0f} VNƒê\")\n",
    "    \n",
    "    # L·ªçc\n",
    "    it_dept = df[df['Department'] == 'IT']\n",
    "    print(f\"\\nNh√¢n vi√™n IT:\\n{it_dept}\")\n",
    "    \n",
    "    # Ghi CSV\n",
    "    df.to_csv('data/employees_copy.csv', index=False)\n",
    "    print(\"\\n‚úÖ ƒê√£ ghi employees_copy.csv\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Pandas ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "    print(\"C√†i ƒë·∫∑t: pip install pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. XML - eXtensible Markup Language\n",
    "\n",
    "**XML** l√† format c√≥ c·∫•u tr√∫c ph√¢n c·∫•p v·ªõi tags, ph·ªï bi·∫øn trong enterprise systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ƒê·ªåC XML V·ªöI ElementTree\n",
    "# ========================================\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# T·∫°o file XML m·∫´u\n",
    "xml_content = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<bookstore>\n",
    "    <book category=\"fiction\">\n",
    "        <title lang=\"en\">Harry Potter</title>\n",
    "        <author>J.K. Rowling</author>\n",
    "        <year>1997</year>\n",
    "        <price>29.99</price>\n",
    "    </book>\n",
    "    <book category=\"programming\">\n",
    "        <title lang=\"en\">Python Crash Course</title>\n",
    "        <author>Eric Matthes</author>\n",
    "        <year>2019</year>\n",
    "        <price>35.99</price>\n",
    "    </book>\n",
    "    <book category=\"programming\">\n",
    "        <title lang=\"vi\">L·∫≠p tr√¨nh Python</title>\n",
    "        <author>Nguy·ªÖn VƒÉn A</author>\n",
    "        <year>2023</year>\n",
    "        <price>150000</price>\n",
    "    </book>\n",
    "</bookstore>'''\n",
    "\n",
    "with open('data/books.xml', 'w', encoding='utf-8') as f:\n",
    "    f.write(xml_content)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o file books.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PARSE XML\n",
    "# ========================================\n",
    "\n",
    "# Parse t·ª´ file\n",
    "tree = ET.parse('data/books.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"=== Parse XML ===\")\n",
    "print(f\"Root tag: {root.tag}\")\n",
    "print(f\"S·ªë books: {len(root)}\")\n",
    "\n",
    "# Duy·ªát qua c√°c books\n",
    "print(\"\\nDanh s√°ch s√°ch:\")\n",
    "for book in root.findall('book'):\n",
    "    category = book.get('category')  # L·∫•y attribute\n",
    "    title = book.find('title').text  # L·∫•y text c·ªßa child element\n",
    "    author = book.find('author').text\n",
    "    price = book.find('price').text\n",
    "    \n",
    "    print(f\"  [{category}] {title} by {author} - ${price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# T√åM KI·∫æM TRONG XML\n",
    "# ========================================\n",
    "\n",
    "print(\"=== T√¨m ki·∫øm XML ===\")\n",
    "\n",
    "# T√¨m t·∫•t c·∫£ books trong category programming\n",
    "print(\"\\n1. S√°ch programming:\")\n",
    "for book in root.findall(\".//book[@category='programming']\"):\n",
    "    print(f\"   - {book.find('title').text}\")\n",
    "\n",
    "# T√¨m theo text content\n",
    "print(\"\\n2. T√¨m t·∫•t c·∫£ authors:\")\n",
    "for author in root.findall('.//author'):\n",
    "    print(f\"   - {author.text}\")\n",
    "\n",
    "# T√¨m book c√≥ title lang='vi'\n",
    "print(\"\\n3. S√°ch ti·∫øng Vi·ªát:\")\n",
    "for book in root.findall(\".//book/title[@lang='vi']/..\"):\n",
    "    print(f\"   - {book.find('title').text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# T·∫†O XML T·ª™ PYTHON\n",
    "# ========================================\n",
    "\n",
    "# T·∫°o root element\n",
    "root = ET.Element('students')\n",
    "root.set('school', 'ABC University')\n",
    "\n",
    "# Th√™m students\n",
    "students_data = [\n",
    "    {'id': '1', 'name': 'An', 'gpa': '8.5', 'major': 'CS'},\n",
    "    {'id': '2', 'name': 'B√¨nh', 'gpa': '9.0', 'major': 'IT'},\n",
    "]\n",
    "\n",
    "for s in students_data:\n",
    "    student = ET.SubElement(root, 'student')\n",
    "    student.set('id', s['id'])\n",
    "    \n",
    "    name = ET.SubElement(student, 'name')\n",
    "    name.text = s['name']\n",
    "    \n",
    "    gpa = ET.SubElement(student, 'gpa')\n",
    "    gpa.text = s['gpa']\n",
    "    \n",
    "    major = ET.SubElement(student, 'major')\n",
    "    major.text = s['major']\n",
    "\n",
    "# Ghi file v·ªõi format ƒë·∫πp\n",
    "ET.indent(root)  # Python 3.9+\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write('data/students.xml', encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(\"=== T·∫°o XML ===\")\n",
    "with open('data/students.xml', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# XML ‚Üî DICT CONVERSION\n",
    "# ========================================\n",
    "\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Chuy·ªÉn XML element th√†nh dict\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Th√™m attributes\n",
    "    if element.attrib:\n",
    "        result['@attributes'] = element.attrib\n",
    "    \n",
    "    # X·ª≠ l√Ω children\n",
    "    children = list(element)\n",
    "    if children:\n",
    "        child_dict = {}\n",
    "        for child in children:\n",
    "            child_data = xml_to_dict(child)\n",
    "            if child.tag in child_dict:\n",
    "                # N·∫øu ƒë√£ c√≥, chuy·ªÉn th√†nh list\n",
    "                if not isinstance(child_dict[child.tag], list):\n",
    "                    child_dict[child.tag] = [child_dict[child.tag]]\n",
    "                child_dict[child.tag].append(child_data)\n",
    "            else:\n",
    "                child_dict[child.tag] = child_data\n",
    "        result.update(child_dict)\n",
    "    elif element.text and element.text.strip():\n",
    "        return element.text.strip()\n",
    "    \n",
    "    return result if result else element.text\n",
    "\n",
    "# Test\n",
    "tree = ET.parse('data/books.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(\"=== XML ‚Üí Dict ===\")\n",
    "import json\n",
    "result = {root.tag: xml_to_dict(root)}\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. YAML - YAML Ain't Markup Language\n",
    "\n",
    "**YAML** l√† format d·ªÖ ƒë·ªçc, ph·ªï bi·∫øn cho config files (Docker, Kubernetes, CI/CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# C√ÄI ƒê·∫∂T PYYAML\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    print(\"‚úÖ PyYAML ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyYAML ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
    "    print(\"Ch·∫°y: pip install pyyaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# YAML C∆† B·∫¢N\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    \n",
    "    # YAML content\n",
    "    yaml_content = '''\n",
    "# Database configuration\n",
    "database:\n",
    "  host: localhost\n",
    "  port: 5432\n",
    "  name: myapp\n",
    "  credentials:\n",
    "    username: admin\n",
    "    password: secret123\n",
    "\n",
    "# Server settings\n",
    "server:\n",
    "  host: 0.0.0.0\n",
    "  port: 8080\n",
    "  debug: true\n",
    "  allowed_hosts:\n",
    "    - localhost\n",
    "    - 127.0.0.1\n",
    "    - example.com\n",
    "\n",
    "# Features\n",
    "features:\n",
    "  enable_cache: true\n",
    "  cache_ttl: 3600\n",
    "  max_connections: 100\n",
    "'''\n",
    "    \n",
    "    # Ghi file\n",
    "    with open('data/config.yaml', 'w', encoding='utf-8') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    # ƒê·ªçc YAML\n",
    "    with open('data/config.yaml', 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"=== YAML ‚Üí Python ===\")\n",
    "    print(f\"Type: {type(config)}\")\n",
    "    print(f\"\\nDatabase host: {config['database']['host']}\")\n",
    "    print(f\"Server port: {config['server']['port']}\")\n",
    "    print(f\"Allowed hosts: {config['server']['allowed_hosts']}\")\n",
    "    print(f\"Cache TTL: {config['features']['cache_ttl']}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è C·∫ßn c√†i ƒë·∫∑t PyYAML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PYTHON ‚Üí YAML\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    \n",
    "    data = {\n",
    "        'application': {\n",
    "            'name': 'MyApp',\n",
    "            'version': '1.0.0',\n",
    "            'authors': ['An', 'B√¨nh', 'C∆∞·ªùng']\n",
    "        },\n",
    "        'settings': {\n",
    "            'theme': 'dark',\n",
    "            'language': 'vi',\n",
    "            'notifications': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Python ‚Üí YAML string\n",
    "    yaml_string = yaml.dump(data, \n",
    "                           allow_unicode=True, \n",
    "                           default_flow_style=False,\n",
    "                           sort_keys=False)\n",
    "    \n",
    "    print(\"=== Python ‚Üí YAML ===\")\n",
    "    print(yaml_string)\n",
    "    \n",
    "    # Ghi file\n",
    "    with open('data/app.yaml', 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(data, f, allow_unicode=True, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ ƒê√£ ghi app.yaml\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è C·∫ßn c√†i ƒë·∫∑t PyYAML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# YAML V·ªöI NHI·ªÄU DOCUMENTS\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    \n",
    "    multi_doc_yaml = '''\n",
    "---\n",
    "name: Document 1\n",
    "type: config\n",
    "---\n",
    "name: Document 2\n",
    "type: data\n",
    "items:\n",
    "  - item1\n",
    "  - item2\n",
    "---\n",
    "name: Document 3\n",
    "type: metadata\n",
    "'''\n",
    "    \n",
    "    # ƒê·ªçc nhi·ªÅu documents\n",
    "    print(\"=== Multiple YAML Documents ===\")\n",
    "    for i, doc in enumerate(yaml.safe_load_all(multi_doc_yaml)):\n",
    "        print(f\"\\nDocument {i + 1}: {doc}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è C·∫ßn c√†i ƒë·∫∑t PyYAML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. INI/Config Files\n",
    "\n",
    "**INI** l√† format ƒë∆°n gi·∫£n cho config files, c√≥ sections v√† key=value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ƒê·ªåC/GHI INI FILES\n",
    "# ========================================\n",
    "import configparser\n",
    "\n",
    "# T·∫°o config\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Th√™m sections v√† values\n",
    "config['DEFAULT'] = {\n",
    "    'debug': 'false',\n",
    "    'log_level': 'INFO'\n",
    "}\n",
    "\n",
    "config['database'] = {\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'name': 'myapp',\n",
    "    'user': 'admin'\n",
    "}\n",
    "\n",
    "config['server'] = {\n",
    "    'host': '0.0.0.0',\n",
    "    'port': '8080',\n",
    "    'debug': 'true'  # Override DEFAULT\n",
    "}\n",
    "\n",
    "# Ghi file\n",
    "with open('data/app.ini', 'w') as f:\n",
    "    config.write(f)\n",
    "\n",
    "print(\"=== INI File ===\")\n",
    "with open('data/app.ini', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ƒê·ªåC INI FILE\n",
    "# ========================================\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('data/app.ini')\n",
    "\n",
    "print(\"=== ƒê·ªçc INI ===\")\n",
    "\n",
    "# Li·ªát k√™ sections\n",
    "print(f\"Sections: {config.sections()}\")\n",
    "\n",
    "# ƒê·ªçc values\n",
    "print(f\"\\nDatabase host: {config['database']['host']}\")\n",
    "print(f\"Database port: {config['database']['port']}\")\n",
    "\n",
    "# ƒê·ªçc v·ªõi type conversion\n",
    "print(f\"\\nServer port (int): {config.getint('server', 'port')}\")\n",
    "print(f\"Server debug (bool): {config.getboolean('server', 'debug')}\")\n",
    "\n",
    "# ƒê·ªçc v·ªõi default\n",
    "print(f\"Timeout (default): {config.get('server', 'timeout', fallback='30')}\")\n",
    "\n",
    "# DEFAULT section ƒë∆∞·ª£c inherit\n",
    "print(f\"\\nLog level (from DEFAULT): {config['database']['log_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# C·∫¨P NH·∫¨T INI FILE\n",
    "# ========================================\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('data/app.ini')\n",
    "\n",
    "# Th√™m section m·ªõi\n",
    "config['cache'] = {\n",
    "    'enabled': 'true',\n",
    "    'ttl': '3600',\n",
    "    'backend': 'redis'\n",
    "}\n",
    "\n",
    "# S·ª≠a gi√° tr·ªã\n",
    "config['server']['port'] = '9000'\n",
    "\n",
    "# X√≥a key\n",
    "if 'debug' in config['server']:\n",
    "    del config['server']['debug']\n",
    "\n",
    "# Ghi l·∫°i\n",
    "with open('data/app.ini', 'w') as f:\n",
    "    config.write(f)\n",
    "\n",
    "print(\"=== Updated INI ===\")\n",
    "with open('data/app.ini', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Data Serialization v·ªõi Pickle\n",
    "\n",
    "**Pickle** cho ph√©p serialize Python objects th√†nh bytes v√† deserialize l·∫°i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PICKLE C∆† B·∫¢N\n",
    "# ========================================\n",
    "import pickle\n",
    "\n",
    "# Data ph·ª©c t·∫°p\n",
    "data = {\n",
    "    'name': 'An',\n",
    "    'scores': [85, 90, 78, 92],\n",
    "    'info': {\n",
    "        'city': 'Hanoi',\n",
    "        'active': True\n",
    "    },\n",
    "    'tags': {'python', 'data', 'ml'}\n",
    "}\n",
    "\n",
    "# Serialize (dump)\n",
    "with open('data/data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ serialize data\")\n",
    "\n",
    "# Deserialize (load)\n",
    "with open('data/data.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\n",
    "print(f\"\\nLoaded data:\")\n",
    "print(f\"  Name: {loaded['name']}\")\n",
    "print(f\"  Scores: {loaded['scores']}\")\n",
    "print(f\"  Tags: {loaded['tags']}\")\n",
    "print(f\"  Same data: {data == loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PICKLE V·ªöI CUSTOM CLASSES\n",
    "# ========================================\n",
    "\n",
    "class Student:\n",
    "    def __init__(self, name, scores):\n",
    "        self.name = name\n",
    "        self.scores = scores\n",
    "    \n",
    "    def average(self):\n",
    "        return sum(self.scores) / len(self.scores)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Student({self.name}, avg={self.average():.1f})\"\n",
    "\n",
    "# T·∫°o objects\n",
    "students = [\n",
    "    Student(\"An\", [85, 90, 78]),\n",
    "    Student(\"B√¨nh\", [92, 88, 95]),\n",
    "    Student(\"C∆∞·ªùng\", [75, 80, 82])\n",
    "]\n",
    "\n",
    "# Serialize\n",
    "with open('data/students.pkl', 'wb') as f:\n",
    "    pickle.dump(students, f)\n",
    "\n",
    "print(\"‚úÖ Serialized students\")\n",
    "\n",
    "# Deserialize\n",
    "with open('data/students.pkl', 'rb') as f:\n",
    "    loaded_students = pickle.load(f)\n",
    "\n",
    "print(\"\\nLoaded students:\")\n",
    "for s in loaded_students:\n",
    "    print(f\"  {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PICKLE V·ªöI BYTES (kh√¥ng c·∫ßn file)\n",
    "# ========================================\n",
    "\n",
    "data = {'key': 'value', 'numbers': [1, 2, 3]}\n",
    "\n",
    "# Serialize th√†nh bytes\n",
    "pickled = pickle.dumps(data)\n",
    "print(f\"=== Pickle bytes ===\")\n",
    "print(f\"Type: {type(pickled)}\")\n",
    "print(f\"Size: {len(pickled)} bytes\")\n",
    "print(f\"Preview: {pickled[:50]}...\")\n",
    "\n",
    "# Deserialize t·ª´ bytes\n",
    "unpickled = pickle.loads(pickled)\n",
    "print(f\"\\nUnpickled: {unpickled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ‚ö†Ô∏è PICKLE SECURITY WARNING\n",
    "# ========================================\n",
    "\n",
    "print(\"=== Pickle Security ===\")\n",
    "print(\"\"\"\n",
    "‚ö†Ô∏è C·∫¢NH B√ÅO AN NINH:\n",
    "\n",
    "1. KH√îNG unpickle data t·ª´ ngu·ªìn kh√¥ng tin c·∫≠y!\n",
    "   - Pickle c√≥ th·ªÉ th·ª±c thi code t√πy √Ω khi load\n",
    "   - Attacker c√≥ th·ªÉ t·∫°o malicious pickle file\n",
    "\n",
    "2. Ch·ªâ d√πng pickle cho:\n",
    "   - Caching trong ·ª©ng d·ª•ng c·ªßa b·∫°n\n",
    "   - L∆∞u ML models (v·ªõi module nh∆∞ joblib)\n",
    "   - IPC gi·ªØa trusted processes\n",
    "\n",
    "3. KH√îNG d√πng pickle cho:\n",
    "   - Data t·ª´ user input\n",
    "   - Data t·ª´ network\n",
    "   - Data sharing v·ªõi untrusted sources\n",
    "\n",
    "4. Alternatives an to√†n h∆°n:\n",
    "   - JSON: cho simple data\n",
    "   - Protocol Buffers: cho structured data\n",
    "   - MessagePack: cho binary data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. L√†m vi·ªác v·ªõi APIs\n",
    "\n",
    "APIs th∆∞·ªùng tr·∫£ v·ªÅ d·ªØ li·ªáu d·∫°ng JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# G·ªåI API V·ªöI REQUESTS (Gi·∫£ l·∫≠p)\n",
    "# ========================================\n",
    "\n",
    "# Gi·∫£ l·∫≠p API response\n",
    "api_response = '''\n",
    "{\n",
    "    \"status\": \"success\",\n",
    "    \"code\": 200,\n",
    "    \"data\": {\n",
    "        \"users\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"Nguy·ªÖn VƒÉn An\",\n",
    "                \"email\": \"an@example.com\",\n",
    "                \"role\": \"admin\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"name\": \"Tr·∫ßn Th·ªã B√¨nh\",\n",
    "                \"email\": \"binh@example.com\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ],\n",
    "        \"total\": 2,\n",
    "        \"page\": 1,\n",
    "        \"per_page\": 10\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse response\n",
    "import json\n",
    "\n",
    "response = json.loads(api_response)\n",
    "\n",
    "print(\"=== API Response Processing ===\")\n",
    "print(f\"Status: {response['status']}\")\n",
    "print(f\"Total users: {response['data']['total']}\")\n",
    "print(\"\\nUsers:\")\n",
    "for user in response['data']['users']:\n",
    "    print(f\"  [{user['id']}] {user['name']} ({user['role']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# API RESPONSE HANDLER\n",
    "# ========================================\n",
    "\n",
    "class APIResponse:\n",
    "    \"\"\"Handler cho API responses\"\"\"\n",
    "    \n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, str):\n",
    "            self._data = json.loads(json_data)\n",
    "        else:\n",
    "            self._data = json_data\n",
    "    \n",
    "    @property\n",
    "    def is_success(self):\n",
    "        return self._data.get('status') == 'success'\n",
    "    \n",
    "    @property\n",
    "    def code(self):\n",
    "        return self._data.get('code', 0)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data.get('data', {})\n",
    "    \n",
    "    def get(self, path, default=None):\n",
    "        \"\"\"Truy c·∫≠p nested data v·ªõi dot notation\"\"\"\n",
    "        keys = path.split('.')\n",
    "        result = self._data\n",
    "        try:\n",
    "            for key in keys:\n",
    "                if key.isdigit():\n",
    "                    result = result[int(key)]\n",
    "                else:\n",
    "                    result = result[key]\n",
    "            return result\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            return default\n",
    "\n",
    "# S·ª≠ d·ª•ng\n",
    "resp = APIResponse(api_response)\n",
    "\n",
    "print(\"=== APIResponse Handler ===\")\n",
    "print(f\"Success: {resp.is_success}\")\n",
    "print(f\"Code: {resp.code}\")\n",
    "print(f\"First user name: {resp.get('data.users.0.name')}\")\n",
    "print(f\"Total: {resp.get('data.total')}\")\n",
    "print(f\"Missing key: {resp.get('data.missing', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# T·∫†O API REQUEST BODY\n",
    "# ========================================\n",
    "\n",
    "def create_api_request(endpoint, method='GET', data=None, headers=None):\n",
    "    \"\"\"T·∫°o API request object\"\"\"\n",
    "    request = {\n",
    "        'endpoint': endpoint,\n",
    "        'method': method,\n",
    "        'headers': headers or {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if data:\n",
    "        request['body'] = json.dumps(data)\n",
    "    \n",
    "    return request\n",
    "\n",
    "# T·∫°o POST request\n",
    "new_user = {\n",
    "    'name': 'L√™ VƒÉn C∆∞·ªùng',\n",
    "    'email': 'cuong@example.com',\n",
    "    'password': 'secret123'\n",
    "}\n",
    "\n",
    "request = create_api_request(\n",
    "    endpoint='/api/users',\n",
    "    method='POST',\n",
    "    data=new_user,\n",
    "    headers={'Authorization': 'Bearer token123'}\n",
    ")\n",
    "\n",
    "print(\"=== API Request ===\")\n",
    "print(json.dumps(request, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Data Validation\n",
    "\n",
    "Validate d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SIMPLE SCHEMA VALIDATION\n",
    "# ========================================\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    pass\n",
    "\n",
    "def validate_schema(data, schema):\n",
    "    \"\"\"Validate data against a simple schema\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for field, rules in schema.items():\n",
    "        value = data.get(field)\n",
    "        \n",
    "        # Required check\n",
    "        if rules.get('required', False) and value is None:\n",
    "            errors.append(f\"{field}: required field missing\")\n",
    "            continue\n",
    "        \n",
    "        if value is None:\n",
    "            continue\n",
    "        \n",
    "        # Type check\n",
    "        expected_type = rules.get('type')\n",
    "        if expected_type and not isinstance(value, expected_type):\n",
    "            errors.append(f\"{field}: expected {expected_type.__name__}, got {type(value).__name__}\")\n",
    "        \n",
    "        # Min/Max for numbers\n",
    "        if isinstance(value, (int, float)):\n",
    "            if 'min' in rules and value < rules['min']:\n",
    "                errors.append(f\"{field}: must be >= {rules['min']}\")\n",
    "            if 'max' in rules and value > rules['max']:\n",
    "                errors.append(f\"{field}: must be <= {rules['max']}\")\n",
    "        \n",
    "        # Min/Max length for strings\n",
    "        if isinstance(value, str):\n",
    "            if 'min_length' in rules and len(value) < rules['min_length']:\n",
    "                errors.append(f\"{field}: minimum length is {rules['min_length']}\")\n",
    "            if 'max_length' in rules and len(value) > rules['max_length']:\n",
    "                errors.append(f\"{field}: maximum length is {rules['max_length']}\")\n",
    "            if 'pattern' in rules:\n",
    "                import re\n",
    "                if not re.match(rules['pattern'], value):\n",
    "                    errors.append(f\"{field}: invalid format\")\n",
    "    \n",
    "    if errors:\n",
    "        raise ValidationError(errors)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Schema definition\n",
    "user_schema = {\n",
    "    'name': {'required': True, 'type': str, 'min_length': 2, 'max_length': 50},\n",
    "    'email': {'required': True, 'type': str, 'pattern': r'^[\\w.-]+@[\\w.-]+\\.\\w+$'},\n",
    "    'age': {'required': False, 'type': int, 'min': 0, 'max': 150},\n",
    "}\n",
    "\n",
    "print(\"=== Schema Validation ===\")\n",
    "\n",
    "# Test cases\n",
    "test_data = [\n",
    "    {'name': 'An', 'email': 'an@test.com', 'age': 25},\n",
    "    {'name': 'B', 'email': 'invalid-email', 'age': 200},\n",
    "    {'email': 'test@test.com'},  # Missing name\n",
    "]\n",
    "\n",
    "for data in test_data:\n",
    "    try:\n",
    "        validate_schema(data, user_schema)\n",
    "        print(f\"‚úÖ Valid: {data}\")\n",
    "    except ValidationError as e:\n",
    "        print(f\"‚ùå Invalid: {data}\")\n",
    "        for error in e.args[0]:\n",
    "            print(f\"   - {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATA CLASS V·ªöI VALIDATION\n",
    "# ========================================\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    name: str\n",
    "    email: str\n",
    "    age: Optional[int] = None\n",
    "    roles: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Validation trong __post_init__\n",
    "        if len(self.name) < 2:\n",
    "            raise ValueError(\"Name must be at least 2 characters\")\n",
    "        if '@' not in self.email:\n",
    "            raise ValueError(\"Invalid email format\")\n",
    "        if self.age is not None and (self.age < 0 or self.age > 150):\n",
    "            raise ValueError(\"Age must be between 0 and 150\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        \"\"\"Create User from dictionary\"\"\"\n",
    "        return cls(\n",
    "            name=data.get('name', ''),\n",
    "            email=data.get('email', ''),\n",
    "            age=data.get('age'),\n",
    "            roles=data.get('roles', [])\n",
    "        )\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary\"\"\"\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'email': self.email,\n",
    "            'age': self.age,\n",
    "            'roles': self.roles\n",
    "        }\n",
    "\n",
    "print(\"=== DataClass with Validation ===\")\n",
    "\n",
    "# Valid user\n",
    "try:\n",
    "    user = User(name=\"An\", email=\"an@test.com\", age=25, roles=['admin'])\n",
    "    print(f\"‚úÖ Created: {user}\")\n",
    "    print(f\"   Dict: {user.to_dict()}\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Invalid user\n",
    "try:\n",
    "    user = User(name=\"A\", email=\"invalid\", age=200)\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# JSON SCHEMA VALIDATION (Basic)\n",
    "# ========================================\n",
    "\n",
    "def validate_json_type(value, type_name):\n",
    "    \"\"\"Validate JSON type\"\"\"\n",
    "    type_map = {\n",
    "        'string': str,\n",
    "        'integer': int,\n",
    "        'number': (int, float),\n",
    "        'boolean': bool,\n",
    "        'array': list,\n",
    "        'object': dict,\n",
    "        'null': type(None)\n",
    "    }\n",
    "    expected = type_map.get(type_name)\n",
    "    if expected is None:\n",
    "        return True\n",
    "    return isinstance(value, expected)\n",
    "\n",
    "# JSON Schema example\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"integer\"},\n",
    "        \"email\": {\"type\": \"string\"},\n",
    "        \"active\": {\"type\": \"boolean\"}\n",
    "    },\n",
    "    \"required\": [\"name\", \"email\"]\n",
    "}\n",
    "\n",
    "def validate_simple_json_schema(data, schema):\n",
    "    \"\"\"Simple JSON schema validation\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check required\n",
    "    for field in schema.get('required', []):\n",
    "        if field not in data:\n",
    "            errors.append(f\"Missing required field: {field}\")\n",
    "    \n",
    "    # Check types\n",
    "    for field, props in schema.get('properties', {}).items():\n",
    "        if field in data:\n",
    "            if not validate_json_type(data[field], props.get('type')):\n",
    "                errors.append(f\"{field}: expected {props.get('type')}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "print(\"=== JSON Schema Validation ===\")\n",
    "\n",
    "test_data = [\n",
    "    {\"name\": \"An\", \"email\": \"an@test.com\", \"age\": 25, \"active\": True},\n",
    "    {\"name\": \"B√¨nh\", \"age\": \"twenty\"},  # Missing email, wrong type\n",
    "]\n",
    "\n",
    "for data in test_data:\n",
    "    errors = validate_simple_json_schema(data, json_schema)\n",
    "    if errors:\n",
    "        print(f\"‚ùå Invalid: {data}\")\n",
    "        for e in errors:\n",
    "            print(f\"   - {e}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Valid: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. ·ª®ng d·ª•ng th·ª±c t·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ·ª®NG D·ª§NG 1: Config Manager\n",
    "# ========================================\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Qu·∫£n l√Ω config t·ª´ nhi·ªÅu ngu·ªìn\"\"\"\n",
    "    \n",
    "    def __init__(self, config_dir='data'):\n",
    "        self.config_dir = Path(config_dir)\n",
    "        self._config = {}\n",
    "    \n",
    "    def load_json(self, filename):\n",
    "        \"\"\"Load JSON config\"\"\"\n",
    "        path = self.config_dir / filename\n",
    "        if path.exists():\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                self._config.update(json.load(f))\n",
    "        return self\n",
    "    \n",
    "    def load_env(self, prefix='APP_'):\n",
    "        \"\"\"Load t·ª´ environment variables\"\"\"\n",
    "        import os\n",
    "        for key, value in os.environ.items():\n",
    "            if key.startswith(prefix):\n",
    "                config_key = key[len(prefix):].lower()\n",
    "                self._config[config_key] = value\n",
    "        return self\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get config value v·ªõi dot notation\"\"\"\n",
    "        keys = key.split('.')\n",
    "        value = self._config\n",
    "        try:\n",
    "            for k in keys:\n",
    "                value = value[k]\n",
    "            return value\n",
    "        except (KeyError, TypeError):\n",
    "            return default\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        \"\"\"Set config value\"\"\"\n",
    "        keys = key.split('.')\n",
    "        config = self._config\n",
    "        for k in keys[:-1]:\n",
    "            config = config.setdefault(k, {})\n",
    "        config[keys[-1]] = value\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save to JSON\"\"\"\n",
    "        path = self.config_dir / filename\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self._config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Demo\n",
    "config = ConfigManager()\n",
    "\n",
    "# Set some values\n",
    "config.set('database.host', 'localhost')\n",
    "config.set('database.port', 5432)\n",
    "config.set('app.name', 'MyApp')\n",
    "config.set('app.debug', True)\n",
    "\n",
    "print(\"=== Config Manager ===\")\n",
    "print(f\"DB Host: {config.get('database.host')}\")\n",
    "print(f\"DB Port: {config.get('database.port')}\")\n",
    "print(f\"App Name: {config.get('app.name')}\")\n",
    "print(f\"Missing: {config.get('app.missing', 'default_value')}\")\n",
    "\n",
    "# Save\n",
    "config.save('app_config.json')\n",
    "print(\"\\n‚úÖ Saved to app_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ·ª®NG D·ª§NG 2: Data Converter\n",
    "# ========================================\n",
    "\n",
    "class DataConverter:\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi gi·ªØa c√°c format\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def csv_to_json(csv_path, json_path=None):\n",
    "        \"\"\"CSV ‚Üí JSON\"\"\"\n",
    "        import csv\n",
    "        \n",
    "        with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            data = list(reader)\n",
    "        \n",
    "        if json_path:\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def json_to_csv(json_path, csv_path):\n",
    "        \"\"\"JSON ‚Üí CSV\"\"\"\n",
    "        import csv\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if not data:\n",
    "            return\n",
    "        \n",
    "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dict_to_xml(data, root_name='root'):\n",
    "        \"\"\"Dict ‚Üí XML string\"\"\"\n",
    "        def _to_xml(d, parent):\n",
    "            if isinstance(d, dict):\n",
    "                for key, val in d.items():\n",
    "                    child = ET.SubElement(parent, key)\n",
    "                    _to_xml(val, child)\n",
    "            elif isinstance(d, list):\n",
    "                for item in d:\n",
    "                    child = ET.SubElement(parent, 'item')\n",
    "                    _to_xml(item, child)\n",
    "            else:\n",
    "                parent.text = str(d) if d is not None else ''\n",
    "        \n",
    "        root = ET.Element(root_name)\n",
    "        _to_xml(data, root)\n",
    "        ET.indent(root)\n",
    "        return ET.tostring(root, encoding='unicode')\n",
    "\n",
    "print(\"=== Data Converter ===\")\n",
    "\n",
    "# CSV ‚Üí JSON\n",
    "json_data = DataConverter.csv_to_json('data/products.csv', 'data/products_converted.json')\n",
    "print(\"CSV ‚Üí JSON:\")\n",
    "print(json.dumps(json_data[:2], indent=2, ensure_ascii=False))\n",
    "\n",
    "# Dict ‚Üí XML\n",
    "sample = {'user': {'name': 'An', 'age': 25, 'skills': ['Python', 'SQL']}}\n",
    "xml_str = DataConverter.dict_to_xml(sample, 'data')\n",
    "print(\"\\nDict ‚Üí XML:\")\n",
    "print(xml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ·ª®NG D·ª§NG 3: Data Pipeline\n",
    "# ========================================\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"Pipeline x·ª≠ l√Ω d·ªØ li·ªáu\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.steps = []\n",
    "    \n",
    "    def filter(self, condition):\n",
    "        \"\"\"L·ªçc data theo ƒëi·ªÅu ki·ªán\"\"\"\n",
    "        self.data = [item for item in self.data if condition(item)]\n",
    "        self.steps.append(f\"filter: {len(self.data)} items\")\n",
    "        return self\n",
    "    \n",
    "    def map(self, transform):\n",
    "        \"\"\"Transform m·ªói item\"\"\"\n",
    "        self.data = [transform(item) for item in self.data]\n",
    "        self.steps.append(\"map\")\n",
    "        return self\n",
    "    \n",
    "    def sort(self, key=None, reverse=False):\n",
    "        \"\"\"S·∫Øp x·∫øp data\"\"\"\n",
    "        self.data = sorted(self.data, key=key, reverse=reverse)\n",
    "        self.steps.append(f\"sort (reverse={reverse})\")\n",
    "        return self\n",
    "    \n",
    "    def take(self, n):\n",
    "        \"\"\"L·∫•y n items ƒë·∫ßu ti√™n\"\"\"\n",
    "        self.data = self.data[:n]\n",
    "        self.steps.append(f\"take: {n}\")\n",
    "        return self\n",
    "    \n",
    "    def result(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ k·∫øt qu·∫£\"\"\"\n",
    "        return self.data\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ th√¥ng tin pipeline\"\"\"\n",
    "        return {\n",
    "            'steps': self.steps,\n",
    "            'result_count': len(self.data)\n",
    "        }\n",
    "\n",
    "# Demo v·ªõi CSV data\n",
    "import csv\n",
    "\n",
    "with open('data/employees.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    employees = list(reader)\n",
    "\n",
    "# Convert salary to int\n",
    "for emp in employees:\n",
    "    emp['Salary'] = int(emp['Salary'])\n",
    "\n",
    "print(\"=== Data Pipeline ===\")\n",
    "print(f\"Original data: {len(employees)} employees\")\n",
    "\n",
    "# Pipeline\n",
    "result = (\n",
    "    DataPipeline(employees)\n",
    "    .filter(lambda x: x['Department'] == 'IT')  # Ch·ªâ IT\n",
    "    .filter(lambda x: x['Salary'] > 10000000)   # L∆∞∆°ng > 10M\n",
    "    .sort(key=lambda x: x['Salary'], reverse=True)  # S·∫Øp x·∫øp gi·∫£m d·∫ßn\n",
    "    .map(lambda x: {'name': x['Name'], 'salary': f\"{x['Salary']:,} VNƒê\"})  # Transform\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline steps: {result.summary()['steps']}\")\n",
    "print(f\"\\nResult:\")\n",
    "for item in result.result():\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ·ª®NG D·ª§NG 4: Log Parser\n",
    "# ========================================\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# T·∫°o sample log\n",
    "log_content = \"\"\"2024-01-15 10:30:45 INFO User login: user_id=123\n",
    "2024-01-15 10:31:02 ERROR Database connection failed: timeout\n",
    "2024-01-15 10:31:15 WARNING High memory usage: 85%\n",
    "2024-01-15 10:32:00 INFO Request processed: endpoint=/api/users, time=150ms\n",
    "2024-01-15 10:32:30 ERROR File not found: config.yaml\n",
    "2024-01-15 10:33:00 INFO User logout: user_id=123\"\"\"\n",
    "\n",
    "with open('data/app.log', 'w') as f:\n",
    "    f.write(log_content)\n",
    "\n",
    "def parse_log(log_file):\n",
    "    \"\"\"Parse log file th√†nh structured data\"\"\"\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (\\w+) (.+)'\n",
    "    entries = []\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            match = re.match(pattern, line.strip())\n",
    "            if match:\n",
    "                timestamp, level, message = match.groups()\n",
    "                entries.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'level': level,\n",
    "                    'message': message\n",
    "                })\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def analyze_logs(entries):\n",
    "    \"\"\"Ph√¢n t√≠ch log entries\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    levels = Counter(e['level'] for e in entries)\n",
    "    \n",
    "    return {\n",
    "        'total': len(entries),\n",
    "        'by_level': dict(levels),\n",
    "        'errors': [e for e in entries if e['level'] == 'ERROR']\n",
    "    }\n",
    "\n",
    "print(\"=== Log Parser ===\")\n",
    "\n",
    "entries = parse_log('data/app.log')\n",
    "analysis = analyze_logs(entries)\n",
    "\n",
    "print(f\"Total entries: {analysis['total']}\")\n",
    "print(f\"By level: {analysis['by_level']}\")\n",
    "print(f\"\\nErrors:\")\n",
    "for error in analysis['errors']:\n",
    "    print(f\"  [{error['timestamp']}] {error['message']}\")\n",
    "\n",
    "# Export to JSON\n",
    "with open('data/log_analysis.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis, f, indent=2)\n",
    "print(\"\\n‚úÖ Exported to log_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. B√†i t·∫≠p th·ª±c h√†nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# B√ÄI T·∫¨P 1: Merge JSON Files\n",
    "# ========================================\n",
    "\n",
    "def merge_json_files(file_paths, output_path):\n",
    "    \"\"\"Merge nhi·ªÅu JSON files th√†nh m·ªôt\"\"\"\n",
    "    merged = []\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged.extend(data)\n",
    "                else:\n",
    "                    merged.append(data)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è File not found: {path}\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Test\n",
    "result = merge_json_files(['data/students.json', 'data/products_converted.json'], 'data/merged.json')\n",
    "print(f\"=== Merged {len(result)} items ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# B√ÄI T·∫¨P 2: CSV Statistics\n",
    "# ========================================\n",
    "\n",
    "def csv_statistics(csv_path):\n",
    "    \"\"\"T√≠nh th·ªëng k√™ t·ª´ CSV file\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    if not rows:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        'row_count': len(rows),\n",
    "        'columns': list(rows[0].keys()),\n",
    "        'numeric_stats': {}\n",
    "    }\n",
    "    \n",
    "    # T√¨m columns s·ªë v√† t√≠nh th·ªëng k√™\n",
    "    for col in stats['columns']:\n",
    "        try:\n",
    "            values = [float(row[col]) for row in rows]\n",
    "            stats['numeric_stats'][col] = {\n",
    "                'min': min(values),\n",
    "                'max': max(values),\n",
    "                'avg': sum(values) / len(values),\n",
    "                'sum': sum(values)\n",
    "            }\n",
    "        except (ValueError, TypeError):\n",
    "            pass  # Not a numeric column\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test\n",
    "stats = csv_statistics('data/employees.csv')\n",
    "print(\"=== CSV Statistics ===\")\n",
    "print(f\"Rows: {stats['row_count']}\")\n",
    "print(f\"Columns: {stats['columns']}\")\n",
    "print(f\"\\nNumeric stats:\")\n",
    "for col, s in stats['numeric_stats'].items():\n",
    "    print(f\"  {col}: min={s['min']}, max={s['max']}, avg={s['avg']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# B√ÄI T·∫¨P 3: Format Detector\n",
    "# ========================================\n",
    "\n",
    "def detect_format(file_path):\n",
    "    \"\"\"Detect file format t·ª´ content\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read(1000)  # ƒê·ªçc 1000 bytes ƒë·∫ßu\n",
    "    \n",
    "    content = content.strip()\n",
    "    \n",
    "    # JSON\n",
    "    if content.startswith('{') or content.startswith('['):\n",
    "        try:\n",
    "            json.loads(content + '...'[:0])  # Just check start\n",
    "            return 'json'\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # XML\n",
    "    if content.startswith('<?xml') or content.startswith('<'):\n",
    "        return 'xml'\n",
    "    \n",
    "    # YAML (check for common patterns)\n",
    "    if ':' in content and not ',' in content.split('\\n')[0]:\n",
    "        return 'yaml'\n",
    "    \n",
    "    # CSV (comma separated)\n",
    "    lines = content.split('\\n')\n",
    "    if len(lines) > 1:\n",
    "        first_line_commas = lines[0].count(',')\n",
    "        if first_line_commas > 0 and all(line.count(',') == first_line_commas for line in lines[:5] if line):\n",
    "            return 'csv'\n",
    "    \n",
    "    # INI\n",
    "    if '[' in content and ']' in content and '=' in content:\n",
    "        return 'ini'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "# Test\n",
    "print(\"=== Format Detector ===\")\n",
    "test_files = ['data/students.json', 'data/employees.csv', 'data/books.xml', 'data/app.ini']\n",
    "\n",
    "for path in test_files:\n",
    "    try:\n",
    "        fmt = detect_format(path)\n",
    "        print(f\"  {path}: {fmt}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  {path}: not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# D·ªåN D·∫∏P\n",
    "# ========================================\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('data')\n",
    "print(\"Files ƒë√£ t·∫°o:\")\n",
    "for f in sorted(data_dir.iterdir()):\n",
    "    print(f\"  {f.name} ({f.stat().st_size} bytes)\")\n",
    "\n",
    "# Uncomment ƒë·ªÉ x√≥a\n",
    "# import shutil\n",
    "# shutil.rmtree('data')\n",
    "# print(\"\\n‚úÖ ƒê√£ x√≥a folder data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## T·ªïng k·∫øt\n",
    "\n",
    "### So s√°nh c√°c formats:\n",
    "\n",
    "| Format | Module | ƒê·ªçc | Ghi | Use Case |\n",
    "|--------|--------|-----|-----|----------|\n",
    "| JSON | `json` | `load/loads` | `dump/dumps` | APIs, Web, Config |\n",
    "| CSV | `csv` | `reader/DictReader` | `writer/DictWriter` | Data tables, Excel |\n",
    "| XML | `xml.etree` | `parse` | `write` | Enterprise, SOAP |\n",
    "| YAML | `pyyaml` | `safe_load` | `dump` | Config, DevOps |\n",
    "| INI | `configparser` | `read` | `write` | Simple config |\n",
    "| Pickle | `pickle` | `load` | `dump` | Python objects |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **JSON**: D√πng `ensure_ascii=False` cho Unicode\n",
    "2. **CSV**: D√πng `newline=''` khi m·ªü file\n",
    "3. **XML**: D√πng `ET.indent()` cho format ƒë·∫πp\n",
    "4. **YAML**: D√πng `safe_load` thay `load`\n",
    "5. **Pickle**: KH√îNG d√πng v·ªõi untrusted data\n",
    "6. **Validation**: Lu√¥n validate data tr∆∞·ªõc khi x·ª≠ l√Ω\n",
    "\n",
    "### Khi n√†o d√πng format n√†o?\n",
    "\n",
    "- **JSON**: APIs, web apps, ƒëa n·ªÅn t·∫£ng\n",
    "- **CSV**: Data analysis, Excel export/import\n",
    "- **XML**: Enterprise systems, legacy APIs\n",
    "- **YAML**: Config files (Docker, K8s, CI/CD)\n",
    "- **INI**: Simple config files\n",
    "- **Pickle**: Caching, ML models, internal use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
